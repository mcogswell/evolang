from collections import OrderedDict
from itertools import product

import numpy as np
import sparse
import pandas as pd
from scipy.misc import logsumexp

from joblib import delayed, Parallel


def entropy(D):
    assert len(D.shape) == 1
    D = D[D > 0]
    return -(D * np.log(D)).sum()

def batch_entropy(D):
    assert len(D.shape) == 2
    dlogd = D * np.log(D)
    dlogd[D == 0] = 0
    return -dlogd.sum(axis=1)

def kl(p1, p2, eps=1e-8):
    mask = (p1 != 0)
    p1 = p1[mask]
    p2 = p2[mask].copy()
    #print(mask, p1, p2) #result)
    #p1[p1 < eps] = eps
    p2[p2 < eps] = eps
    result = (p1 * (np.log(p1) - np.log(p2))).sum()
    #print(result)
    # TODO: is this stuff right?
    assert result >= 0
    return result

def ce_pdist(p1, p2, eps=1e-8):
    assert len(p1.shape) == len(p2.shape) == 1
    p1[p1 < eps] = eps
    p2[p2 < eps] = eps
    ce1 = -(p1 * np.log(p2)).sum()
    ce2 = -(p2 * np.log(p1)).sum()
    return (ce1 + ce2) / 2

def kl_pdist(p1, p2):
    assert len(p1.shape) == len(p2.shape) == 1
    return (kl(p1, p2) + kl(p2, p1)) / 2

pdist = kl_pdist



class LanguageAnalyzer(object):

    def __init__(self, exp_dict, split='train'):
        '''
        This provides tools for analyzing language generated by a set of
        experiments

        NOTE: The metrics here depend in minor ways on the entire set of
        experiments passed through exp_dict. So even if the results for one
        experiment are the same then the metrics might still be different
        if the other experiments in the set are different. This is because
        The entire set of experiments is used to enumerate the space of possible
        conversations. TODO: check this again...

        exp_dict: a dict from experiment codes to raw json evaluation file data
        split: a string or list of strings in ['train', 'val', 'test']
        '''
        splits = [split] if isinstance(split, str) else split
        self.splits = splits

        # convert experiment results to a nice pd.DataFrame
        self.lang_records = self.exp_results_to_lang_records(exp_dict, splits)
        ldf = pd.DataFrame.from_records(self.lang_records,
                                    columns=self.lang_columns)
        self.all_exp_ldf = ldf

        # enumerate the space of possible observations and conversations
        # column map (column name -> list of unique values (i.e. idx -> value))
        # NOTE: this is the part that depends on the entire set of experiments
        cm = OrderedDict([
            ('task1', ldf.task1.unique()),
            ('task2', ldf.task2.unique()),
            ('color', ldf.color.unique()),
            ('shape', ldf['shape'].unique()),
            ('style', ldf['style'].unique()),
            ('q1', ldf.q1.unique()),
            ('a1', ldf.a1.unique()),
            ('q2', ldf.q2.unique()),
            ('a2', ldf.a2.unique()),
        ])
        self.cm = cm
        # column name -> (value -> idx of value)
        self.cm_inv = OrderedDict((col, {v: ix for ix, v in enumerate(values)})
                                                  for col, values in cm.items())
        self.cm_dims = OrderedDict((k, i) for i, k in enumerate(cm))
        self.cm_shape = (len(cm['task1']), len(cm['task2']),
                         len(cm['color']), len(cm['shape']), len(cm['style']),
                         len(cm['q1']), len(cm['a1']),
                         len(cm['q2']), len(cm['a2']))

        # compute empirical pmfs for all conversation/observation data
        self.experiments = {}
        for exp_code in exp_dict:
            print('loading {}'.format(exp_code))
            self.compute_pmfs(exp_code)

    @classmethod
    def exp_results_to_lang_records(cls, exp_dict, splits):
        lang_records = []
        for exp_code, d in exp_dict.items():
            records = cls.process_exp_result(d, exp_code, splits)
            lang_records.extend(records)
        return lang_records

    lang_columns = ['exp', 'split', 'qboti', 'aboti', 'color', 'shape', 'style', 'task1', 'task2', 'q1', 'a1', 'q2', 'a2', 'p1', 'p2', 'samplei', 'epoch', 'q_generation', 'a_generation', 'q_age', 'a_age']

    # NOTE: This and exp_results_to_lang_records are class functions because
    # they depend on the list of column names which is a class attribute.
    @classmethod
    def process_exp_result(cls, data, exp_code, splits):
        lang_records = []
        for epoch in data.keys():
            Nq, Na = data[epoch]['qbots'], data[epoch]['abots']
            # unknown by default
            default_ages = {'qbot': [-1] * Nq, 'abot': [-1] * Na}
            default_generations = {'qbot': [-1] * Nq, 'abot': [-1] * Na}
            for qi, ai in product(range(Nq), range(Na)):
                q_generation = data[epoch].get('generations', default_generations)['qbot'][qi]
                a_generation = data[epoch].get('generations', default_generations)['abot'][ai]
                q_age = data[epoch].get('ages', default_ages)['qbot'][qi]
                a_age = data[epoch].get('ages', default_ages)['abot'][ai]
                for split in splits:
                    if data[epoch].get('how_to_sample', 0) == 0:
                        samples = [data[epoch][split]['script'][qi][ai]]
                    else:
                        nsamples = len(data[epoch][split])
                        samples = [data[epoch][split][i]['script'][qi][ai] for i in range(nsamples)]
                    for sample_i, sample in enumerate(samples):
                        # one example per task/object observation in the split
                        for ex in sample:
                            image = ex['image']
                            task = ex['task']
                            chat = ex['chat']
                            pred = ex['pred']
                            r = (exp_code, split, qi, ai,
                                 image[0], image[1], image[2],
                                 task[0], task[1],
                                 chat[0], chat[1], chat[2], chat[3],
                                 pred[0], pred[1],
                                 sample_i, epoch,
                                 q_generation, a_generation,
                                 q_age, a_age)
                            lang_records.append(r)
        return lang_records

    def compute_pmfs(self, exp_code):
        ldf = self.all_exp_ldf[self.all_exp_ldf.exp == exp_code]
        pmfs = self.extract_interesting_pmfs(ldf)
        self.experiments[exp_code] = {
            'pmfs': pmfs,
            'trans_dists': {},
        }

    def extract_interesting_pmfs(self, ldf):
        '''
        Compute a bunch of empirical probability mass functions for the
        language data frame of a selected set of observations (usually from
        one experiment).
        '''
        NQ = len(ldf.qboti.unique())
        NA = len(ldf.aboti.unique())
        pmfs = {
            'count':      [[None for _ in range(NA)] for _ in range(NQ)],
            'joint':      [[None for _ in range(NA)] for _ in range(NQ)],
            'qd1':        [[None for _ in range(NA)] for _ in range(NQ)],
            'qd2':        [[None for _ in range(NA)] for _ in range(NQ)],
            'ad1':        [[None for _ in range(NA)] for _ in range(NQ)],
            'ad2':        [[None for _ in range(NA)] for _ in range(NQ)],
            'ad1_noq':    [[None for _ in range(NA)] for _ in range(NQ)],
            'ad2_noq':    [[None for _ in range(NA)] for _ in range(NQ)],
            'task_joint': [[None for _ in range(NA)] for _ in range(NQ)],
            'obj_joint':  [[None for _ in range(NA)] for _ in range(NQ)],
            'obj_joint_q1':  [[None for _ in range(NA)] for _ in range(NQ)],
            'obj_joint_q2':  [[None for _ in range(NA)] for _ in range(NQ)],
        }
        condition = self.condition
        cm_dims = self.cm_dims

        for qi, ai in product(range(NQ), range(NA)):
            # pmfs for direct language similarity metric
            # not yet normalized!!
            c = self.df_to_counts(ldf, qi, ai)
            p = c / c.sum()
            qd1 = condition(c, (cm_dims['q1'],), (cm_dims['task1'], cm_dims['task2']))
            qd2 = condition(c, (cm_dims['q2'],), (cm_dims['task1'], cm_dims['task2']))
            ad1 = condition(c, (cm_dims['a1'],), (cm_dims['color'], cm_dims['shape'], cm_dims['style'], cm_dims['q1']))
            ad2 = condition(c, (cm_dims['a2'],), (cm_dims['color'], cm_dims['shape'], cm_dims['style'], cm_dims['q2']))
            ad1_noq = condition(c, (cm_dims['a1'],), (cm_dims['color'], cm_dims['shape'], cm_dims['style']))
            ad2_noq = condition(c, (cm_dims['a2'],), (cm_dims['color'], cm_dims['shape'], cm_dims['style']))
            task_joint = condition(c, (cm_dims['task1'], cm_dims['task2']), ())
            obj_joint = condition(c, (cm_dims['color'], cm_dims['shape'], cm_dims['style']), ())
            obj_joint_q1 = condition(c, (cm_dims['color'], cm_dims['shape'], cm_dims['style'], cm_dims['q1']), ())
            obj_joint_q2 = condition(c, (cm_dims['color'], cm_dims['shape'], cm_dims['style'], cm_dims['q2']), ())
            pmfs['count'][qi][ai] = c
            pmfs['joint'][qi][ai] = p
            pmfs['qd1'][qi][ai] = qd1
            pmfs['qd2'][qi][ai] = qd2
            pmfs['ad1'][qi][ai] = ad1
            pmfs['ad2'][qi][ai] = ad2
            pmfs['ad1_noq'][qi][ai] = ad1_noq
            pmfs['ad2_noq'][qi][ai] = ad2_noq
            pmfs['task_joint'][qi][ai] = task_joint
            pmfs['obj_joint'][qi][ai] = obj_joint
            pmfs['obj_joint_q1'][qi][ai] = obj_joint_q1
            pmfs['obj_joint_q2'][qi][ai] = obj_joint_q2

        # sanity check
        task_joint = pmfs['task_joint'][0][0]
        obj_joint = pmfs['obj_joint'][0][0]
        # check that this is the same for all
        for i in range(NQ):
            for j in range(NA):
                assert np.all(pmfs['task_joint'][i][j] == task_joint)
                assert np.all(pmfs['obj_joint'][i][j] == obj_joint)
        pmfs['task_joint'] = task_joint
        pmfs['obj_joint'] = obj_joint

        # pmfs for neuralese translation stuff
        # https://arxiv.org/pdf/1704.06960.pdf
        # Note: I don't actually need to most of the work they do.
        # I can just compute stuff that is intractable for them since I'm using
        # empirical distributions. I directly implement the 3rd line of eq. 1.
        # Each bot utters two tokens (q1_q2 or a1_a2). I can translate between
        # individual words (e.g., q1 from different bots) or pairs of words
        # (e.g., q1_q2 from different bots). To do so I just need to compute
        # the translation distributions using slightly different variables
        # for z and the code below can do all those versions.
        pmfs['trans'] = {}

        # Q-bot speaker, A-bot listener
        #pmfs['trans']['q1'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['q1'],),
        #    sp_state_dims=(cm_dims['task1'], cm_dims['task2']),
        #    l_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']))
        #pmfs['trans']['q2'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['q2'],),
        #    sp_state_dims=(cm_dims['task1'], cm_dims['task2']),
        #    l_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']))
        #pmfs['trans']['q1_q2'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['q1'], cm_dims['q2']),
        #    sp_state_dims=(cm_dims['task1'], cm_dims['task2']),
        #    l_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']))

        ## A-bot speaker, Q-bot listener
        #pmfs['trans']['a1'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['a1'],),
        #    sp_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']),
        #    l_state_dims=(cm_dims['task1'], cm_dims['task2']))
        #pmfs['trans']['a2'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['a2'],),
        #    sp_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']),
        #    l_state_dims=(cm_dims['task1'], cm_dims['task2']))
        #pmfs['trans']['a1_a2'] = self.translation_pmfs(pmfs,
        #    z_dims=(cm_dims['a1'], cm_dims['a2'],),
        #    sp_state_dims=(cm_dims['color'], cm_dims['shape'], cm_dims['style']),
        #    l_state_dims=(cm_dims['task1'], cm_dims['task2']))

        return pmfs

    def df_to_counts(self, df, qbotidx, abotidx):
        coords = []
        df = df[df.qboti == qbotidx]
        df = df[df.aboti == abotidx]
        for _, row in df.iterrows():
            idx = tuple(self.cm_inv[k][row[k]] for k in self.cm)
            coords.append(idx)
        #assert len(set(coords)) == len(coords)
        coords = list(zip(*coords))
        p = sparse.COO(coords, data=1., shape=self.cm_shape)
        return p

    def condition(self, p, p_dims, cond_dims, eps=1e-8):
        # Note that the joint p can be unnormalized (array of counts)
        p_dims = tuple(p_dims)
        cond_dims = tuple(cond_dims)
        marginalize_dims = tuple(dim for dim in self.cm_dims.values()
                                     if dim not in set(p_dims + cond_dims))
        assert (set(p_dims + cond_dims + marginalize_dims) == 
                set(self.cm_dims.values()))
        p = p.sum(axis=marginalize_dims, keepdims=True)
        Z = p.sum(axis=p_dims, keepdims=True)
        p = p / (Z + eps)
        # TODO: might be more efficient here depending on downstream uses
        p = p.todense().transpose(p_dims + cond_dims + marginalize_dims)
        return p.squeeze()

    def translation_pmfs(self, pmfs, z_dims, sp_state_dims, l_state_dims):
        result = {}
        result['sp_state_dims'] = sp_state_dims
        result['l_state_dims'] = l_state_dims
        result['z_dims'] = z_dims
        state_dims = sp_state_dims + l_state_dims
        result['state_dims'] = state_dims
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        condition = self.condition

        # In the paper's notation:
        #   sp = a = speaker
        #   l = b = listener
        # p(x_sp, x_l)
        result['p_xsp_xl'] = [[None for _ in range(NA)] for _ in range(NQ)]
        # p(z | x_sp, x_l)
        result['pz_xsp_xl'] = [[None for _ in range(NA)] for _ in range(NQ)]
        # p(z | x_sp)
        result['pz_xsp'] = [[None for _ in range(NA)] for _ in range(NQ)]
        # p(x_sp | z, x_l)
        result['belief'] = [[None for _ in range(NA)] for _ in range(NQ)]
        # p(z)
        result['z'] = [[None for _ in range(NA)] for _ in range(NQ)]
        # Note: p(z' | x_sp) may seem to be missing here, but it is just some
        # other choice of qi or ai

        for qi, ai in product(range(NQ), range(NA)):
            count = pmfs['count'][qi][ai]
            # p(z | x_sp, x_l)
            result['pz_xsp_xl'][qi][ai] = condition(count, z_dims, sp_state_dims + l_state_dims)
            # p(z | x_sp)
            result['pz_xsp'][qi][ai] = condition(count, z_dims, sp_state_dims)
            # p(x_sp | z, x_l)
            belief = condition(count, sp_state_dims, z_dims + l_state_dims)
            result['belief'][qi][ai] = belief
            # p(z)
            result['z'][qi][ai] = condition(count, z_dims, ())

        # p(X_sp, X_l)
        # NOTE: I've verified this is the same for all bots, as it should be.
        result['p_xsp_xl'] = condition(pmfs['count'][0][0], state_dims, ())
        return result

    def lang_quality(self, exp_code, bot_type='qbot'):
        '''
        Measure the entropy of bot utterances given the approriate context.
        Appropriate context means that the context captures only and all of
        the observation that needs to be communicated.
        If entropy is low then they only say one thing in each context and
        the mapping between language and concepts is good (because the
        context is appropriate).
        '''
        assert bot_type == 'abot'
        if 'lang_quality' in self.experiments[exp_code]:
            return self.experiments[exp_code]['lang_quality']
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        H = []
        for ai in range(NA):
            H_ai = 0
            for qi in range(NQ):
                ad1 = pmfs['ad1'][qi][ai]
                ad1 = ad1.reshape(ad1.shape[0], -1).transpose(1, 0)
                with np.errstate(divide='ignore'):
                    H_ai += batch_entropy(ad1).mean()
                ad2 = pmfs['ad2'][qi][ai]
                ad2 = ad2.reshape(ad2.shape[0], -1).transpose(1, 0)
                with np.errstate(divide='ignore'):
                    H_ai += batch_entropy(ad2).mean()
            H_ai /= 2 * NQ
            H.append(H_ai)
        return np.mean(H)

    def cross_abot_match_freq(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        if NA == 1:
            return 0.

        a1_matches = 0
        a2_matches = 0
        total = 0
        ldf = self.all_exp_ldf
        ldf = ldf[ldf.exp == exp_code]
        for qi in range(NQ):
            for ai1 in range(NA):
                for ai2 in range(NA):
                    if ai1 == ai2:
                        continue
                    ldfq = ldf[ldf.qboti == qi]
                    ldf1 = ldfq[ldfq.aboti == ai1]
                    ldf2 = ldfq[ldfq.aboti == ai2]
                    a1_matches += (ldf1.a1.reset_index(drop=True) == \
                                   ldf2.a1.reset_index(drop=True)).sum()
                    a2_matches += (ldf1.a2.reset_index(drop=True) == \
                                   ldf2.a2.reset_index(drop=True)).sum()
                    total += ldf1.a2.shape[0]
        a1_avg = a1_matches / total
        a2_avg = a2_matches / total
        return (a1_avg + a2_avg) / 2

    def lang_dists(self, exp_code, bot_type='qbot', cond_word=False):
        '''
        Measure whether bots tend to say the same thing in the same context
        or not.

        bot_type: either 'qbot' or 'abot'
        cond_word: if True, context includes last utterance in the conversation
                   otherwise, context includes just relevant observations
        '''
        # D^Q_{ij}
        if 'lang_dists' in self.experiments[exp_code]:
            return self.experiments[exp_code]['lang_dists']
        if   bot_type == 'qbot' and not cond_word:
            return self._lang_dists_qbot(exp_code)
        elif bot_type == 'qbot' and cond_word:
            return self._lang_dists_qbot_with_aword(exp_code)
        elif bot_type == 'abot' and not cond_word:
            return self._lang_dists_abot(exp_code)
        elif bot_type == 'abot' and cond_word:
            return self._lang_dists_abot_with_qword(exp_code)
        else:
            raise Exception('Unknown bot type {}. '
                            'Choose qbot or abot.'.format(bot_type))

    def _lang_dists_qbot(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        task_joint = pmfs['task_joint']
        lang_dists = [[None for _ in range(NQ)] for _ in range(NQ)]
        for qi, qj in product(range(NQ), range(NQ)):
            lang_dist = 0.
            for ak in range(NA):
                for ctx in map(tuple, np.transpose(np.nonzero(task_joint))):
                    t1i, t2i = ctx
                    p_task = task_joint[ctx]
                    lang_dist += p_task * pdist(pmfs['qd1'][qi][ak][:, t1i, t2i], pmfs['qd1'][qj][ak][:, t1i, t2i])
                    lang_dist += p_task * pdist(pmfs['qd2'][qi][ak][:, t1i, t2i], pmfs['qd2'][qj][ak][:, t1i, t2i])
            lang_dist /= 2 * NA
            lang_dists[qi][qj] = lang_dist
        lang_dists = np.array(lang_dists)
        self.experiments[exp_code]['qbot_lang_dists'] = lang_dists
        return lang_dists

    def _lang_dists_qbot_with_aword(self, exp_code):
        assert False
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        task_joint = pmfs['task_joint']
        lang_dists = [[None for _ in range(NQ)] for _ in range(NQ)]
        for qi, qj in product(range(NQ), range(NQ)):
            lang_dist = 0.
            for ak in range(NA):
                for ctx in map(tuple, np.transpose(np.nonzero(task_joint))):
                    t1i, t2i = ctx
                    p_task = task_joint[ctx]
                    lang_dist += p_task * pdist(pmfs['qd1'][qi][ak][:, t1i, t2i], pmfs['qd1'][qj][ak][:, t1i, t2i])
                    lang_dist += p_task * pdist(pmfs['qd2'][qi][ak][:, t1i, t2i], pmfs['qd2'][qj][ak][:, t1i, t2i])
            lang_dist /= 2 * NA
            lang_dists[qi][qj] = lang_dist
        lang_dists = np.array(lang_dists)
        self.experiments[exp_code]['qbot_aword_lang_dists'] = lang_dists
        return lang_dists

    def _lang_dists_abot(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        obj_joint = pmfs['obj_joint']
        lang_dists = [[None for _ in range(NA)] for _ in range(NA)]
        for ai, aj in product(range(NA), range(NA)):
            lang_dist = 0.
            # TODO: have a version that conditions on qbot word and a version that just conditions on context
            for qk in range(NQ):
                for ctx in map(tuple, np.transpose(np.nonzero(obj_joint))):
                    color, shape, style = ctx
                    p_obj = obj_joint[ctx]
                    lang_dist += p_obj * pdist(pmfs['ad1_noq'][qk][ai][:, color, shape, style], pmfs['ad1_noq'][qk][aj][:, color, shape, style])
                    lang_dist += p_obj * pdist(pmfs['ad2_noq'][qk][ai][:, color, shape, style], pmfs['ad2_noq'][qk][aj][:, color, shape, style])
            lang_dist /= 2 * NQ
            lang_dists[ai][aj] = lang_dist
        lang_dists = np.array(lang_dists)
        self.experiments[exp_code]['abot_lang_dists'] = lang_dists
        return lang_dists

    def _lang_dists_abot_with_qword(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        lang_dists = [[None for _ in range(NA)] for _ in range(NA)]
        for ai, aj in product(range(NA), range(NA)):
            lang_dist = 0.
            # TODO: have a version that conditions on qbot word and a version that just conditions on context
            for qk in range(NQ):
                # first round of dialog
                obj_joint_q1_i = pmfs['obj_joint_q1'][qk][ai]
                obj_joint_q1_j = pmfs['obj_joint_q1'][qk][aj]
                assert (obj_joint_q1_i == obj_joint_q1_j).all()
                # TODO: see if these are the same for the first round... they should be given qbot...
                #                for ctx in map(tuple, np.transpose(np.nonzero(obj_joint))):
                #                    color, shape, style = ctx
                #                    for q1 in map(tuple, np.transpose(np.nonzero(qk_first_words))):
                #                    for q1 in possible_q1s from uniform distribution over q1:
                #                        p(color, shape, style, q1)
                #
                #
                #                        conclusion: switch to sampling, do the old metric... think more... also report conditioning on q1 with sampling... then think about the difference in indices
                #
                #                        p(q2 | q1, a1, color, shape, style) * p(q1, a1, color, shape, style) * p(color, shape, style)
                #                        p(q2, q1, a1, color, shape, style)
                #
                #
                #
                #                        sampling properly:
                #                            E[ KL(p(a1_i | color, shape, style), p(a1_j | color, shape, style)) ] + \
                #                            E[ KL(p(a2_i | color, shape, style), p(a2_j | color, shape, style)) ]
                #
                #                            E[ KL(p(a1_i | q1, color, shape, style), p(a1_j | q1, color, shape, style)) ]
                #
                #                            compare single agents across time
                #                                abot i right before it's killed
                #                                    vs abot i+1 right before it's killed
                #                                    vs abot i+1 right before qbot is killed
                #
                #
                #                            p(a1_i, a2_i | color, shape, style)
                #
                #
                #
                #                            p(a1_i | color, shape, style)
                #                            p(a1_i | q1, color, shape, style)
                #
                #
                #                            p(a2_i | q2_i, color, shape, style)
                #                            p(a2_i | q2_i, q1, a1_i, color, shape, style)
                #
                #
                #                        first round:
                #                        E_?? [ KL(p(a1_i | q1, color, shape, style), p(a1_j | q1, color, shape, style)) ]
                #
                #
                #
                #                        second round:
                #                        E_?? [ KL(p(a2_i | q2_i, q1, a1_i, color, shape, style), p(a2_j | q2_j, q1, a1_j, color, shape, style)) ]
                #
                #                        =?
                #
                #                        E_?? [ KL(p(a2_i | color, shape, style), p(a2_j | color, shape, style)) ]
                #
                #                        p(color, shape, style) * p(q1)
                #                        p_ctx = obj_joint_q1[ctx]
                #                        lang_dist += p_ctx * pdist(pmfs['ad1'][qk][ai][:, color, shape, style, q1], pmfs['ad1'][qk][aj][:, color, shape, style, q1])
                #                    for q2 in map(tuple, np.transpose(np.nonzero(qk_second_words))):
                #
                #
                #
                ## second round of dialog
                #obj_joint_q2 = pmfs['obj_joint_q2']
                #for ctx in map(tuple, np.transpose(np.nonzero(obj_joint_q2))):
                #    color, shape, style, q2 = ctx
                #    p_ctx = obj_joint_q2[ctx]
                #    lang_dist += p_ctx * pdist(pmfs['ad2'][qk][ai][:, color, shape, style, q2], pmfs['ad2'][qk][aj][:, color, shape, style, q2])
            lang_dist /= 2 * NQ
            lang_dists[ai][aj] = lang_dist
        lang_dists = np.array(lang_dists)
        self.experiments[exp_code]['abot_with_qword_lang_dists'] = lang_dists
        return lang_dists

    def lang_entropy(self, exp_code, bot_type='qbot'):
        '''
        Measure whether bots tend to say the same thing in the same context
        or not.

        bot_type: either 'qbot' or 'abot'
        cond_word: if True, context includes last utterance in the conversation
                   otherwise, context includes just relevant observations
        '''
        # D^Q_{ij}
        if   bot_type == 'qbot':
            return self._lang_entropy_qbot(exp_code)
        elif bot_type == 'abot':
            return self._lang_entropy_abot(exp_code)
        else:
            raise Exception('Unknown bot type {}. '
                            'Choose qbot or abot.'.format(bot_type))

    def _lang_entropy_abot(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        obj_joint = pmfs['obj_joint']
        entropies = [None for _ in range(NA)]
        for ai in range(NA):
            ent = 0.
            # TODO: have a version that conditions on qbot word and a version that just conditions on context
            for qk in range(NQ):
                for ctx in map(tuple, np.transpose(np.nonzero(obj_joint))):
                    color, shape, style = ctx
                    p_obj = obj_joint[ctx]
                    ent += p_obj * entropy(pmfs['ad1_noq'][qk][ai][:, color, shape, style])
                    ent += p_obj * entropy(pmfs['ad2_noq'][qk][ai][:, color, shape, style])
            ent /= 2 * NQ
            entropies[ai] = ent
        entropies = np.array(entropies)
        self.experiments[exp_code]['abot_entropies'] = entropies
        return entropies

    def _lang_entropy_qbot(self, exp_code):
        pmfs = self.experiments[exp_code]['pmfs']
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        task_joint = pmfs['task_joint']
        entropies = [None for _ in range(NQ)]
        for qi in range(NQ):
            ent = 0.
            for ak in range(NA):
                for ctx in map(tuple, np.transpose(np.nonzero(task_joint))):
                    t1i, t2i = ctx
                    p_task = task_joint[ctx]
                    ent += p_task * entropy(pmfs['qd1'][qi][ak][:, t1i, t2i])
                    ent += p_task * entropy(pmfs['qd2'][qi][ak][:, t1i, t2i])
            ent /= 2 * NQ
            entropies[qi] = ent
        entropies = np.array(entropies)
        self.experiments[exp_code]['qbot_entropies'] = entropies
        return entropies

    def trans_dists(self, exp_code=None, zkey='q1_q2'):
        return_type = 'dict'
        if exp_code == None:
            exp_codes = list(self.experiments.keys())
        elif isinstance(exp_code, str):
            exp_codes = [exp_code]
            return_type = 'single'
        else:
            exp_codes = exp_code

        compute_exp_codes = []
        for exp_code in exp_codes:
            assert exp_code in self.experiments
            if 'trans_dists' in self.experiments[exp_code]:
                if zkey in self.experiments[exp_code]['trans_dists']:
                    continue
            compute_exp_codes.append(exp_code)

        #n_jobs = min(15, len(compute_exp_codes))...
        # Parallel made everything worse because of data transfer costs
        with Parallel(n_jobs=1) as parallel:
            dists_lst = parallel(delayed(self.trans_dist)(self.experiments[exp_code]['pmfs'], zkey) for exp_code in compute_exp_codes)

        for dists, exp_code in zip(dists_lst, compute_exp_codes):
            self.experiments[exp_code]['trans_dists'][zkey] = dists

        if return_type == 'dict':
            result = {}
            for exp_code in exp_codes:
                result[exp_code] = self.experiments[exp_code]['trans_dists'][zkey]
            return result
        else:
            return self.experiments[exp_code]['trans_dists'][zkey]

    def trans_dist(self, pmfs, zkey):
        # TODO: make this bot-agnostic
        NQ = len(pmfs['count'])
        NA = len(pmfs['count'][0])
        dists = np.zeros([NQ, NQ])
        trans_pmfs = pmfs['trans'][zkey]
        z_dims = trans_pmfs['z_dims']
        sent_lst = list(product(*[range(self.cm_shape[d]) for d in z_dims]))
        def was_spoken(z, qi, ai):
            return trans_pmfs['z'][qi][ai][z] > 0

        for ai in range(NA):
            for qi in range(NQ):
                for qj in range(NQ):
                    if qi == qj:
                        continue
                    qvals = []
                    weights = []
                    for senti in sent_lst:
                        if not was_spoken(senti, qi, ai):
                            continue
                        w = trans_pmfs['z'][qi][ai][senti]
                        if w == 0:
                            continue
                        else:
                            weights.append(w)
                        qhats = [self.qhat(senti, sentj, qi=qi, qpi=qj, ai=ai, pmfs=trans_pmfs)
                                        for sentj in sent_lst if was_spoken(sentj, qj, ai)]
                        q = min(qhats)
                        qvals.append(q)
                    dists[qi,qj] += (np.array(weights) * np.array(qvals)).sum()

        dists /= NA
        return dists

    def qhat(self, z, zp, qi, qpi, ai, pmfs, eps=1e-8):
        '''
        Compute the sum on the 3rd line of eq. 1 of the neuralese paper more
        or less directly using empirical distributions.
        It takes the expectation (over actual observed states (listener and
        speaker state) of the world) of the difference between
        speaker states imagined for z and for z'.
        '''
        raise Exception('Uncomment the relevant translation_pmfs() calls at '
                        'the bottom of extract_interesting_pmfs() to make this '
                        'work again. They are commented out because they are '
                        'really slow.')
        pz_xsp = pmfs['pz_xsp'][qi][ai]
        pzp_xsp = pmfs['pz_xsp'][qpi][ai]
        belief = pmfs['belief'][qi][ai]
        beliefp = pmfs['belief'][qpi][ai]
        p_xsp_xl = pmfs['p_xsp_xl']
        state_dims = pmfs['state_dims']
        n_sp_state_dims = len(pmfs['sp_state_dims'])
        colon = (slice(None),)
        colons = colon * n_sp_state_dims
        state_shape = [self.cm_shape[d] for d in state_dims]
        n_states = int(np.prod(state_shape))

        # log( p(x_a, x_b) * p(z | x_a) * p(z' | x_a) )
        log_wis = np.zeros(n_states)
        # D_KL(beta(z, x_b) || beta(z', x_b))
        kis = np.zeros(n_states)

        # compute a weighted average of divergences over all states
        state_iter = product(*[range(d) for d in state_shape])
        for i, x in enumerate(state_iter):
            xsp = x[:n_sp_state_dims]
            xl = x[n_sp_state_dims:]

            # compute weights
            p1 = p_xsp_xl[x]
            p2 = pz_xsp[z + xsp]
            p3 = pzp_xsp[zp + xsp]
            # No use including x in the sum because it would be 0 anyway
            if p1 * p2 * p3 == 0:
                continue
            log_wi = np.log(p1) + np.log(p2) + np.log(p3)
            log_wis[i] = log_wi

            # compute divergences
            b = belief[colons + z + xl]
            bp = beliefp[colons + zp + xl]
            #assert np.all(np.isclose(b.sum(), 1.))
            #assert np.all(np.isclose(bp.sum(), 1.))
            ki = kl(b, bp)
            kis[i] = ki

        wis = np.exp(log_wis - logsumexp(log_wis))
        #assert np.isclose(wi.sum(), 1.0)
        return kis.dot(wis)

